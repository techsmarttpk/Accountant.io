{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9410484,"sourceType":"datasetVersion","datasetId":5714317}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/pranavkrishnaprathap/gpt2training?scriptVersionId=197029735\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"!pip install transformers","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-17T14:34:55.04732Z","iopub.execute_input":"2024-09-17T14:34:55.047666Z","iopub.status.idle":"2024-09-17T14:35:09.92581Z","shell.execute_reply.started":"2024-09-17T14:34:55.047628Z","shell.execute_reply":"2024-09-17T14:35:09.924825Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.44.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.24.6)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.4)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.7.4)\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import GPT2Tokenizer, GPT2LMHeadModel\n\n# Load pre-trained model and tokenizer\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\nmodel = GPT2LMHeadModel.from_pretrained('gpt2')\n","metadata":{"execution":{"iopub.status.busy":"2024-09-17T14:35:20.08105Z","iopub.execute_input":"2024-09-17T14:35:20.081746Z","iopub.status.idle":"2024-09-17T14:35:30.774994Z","shell.execute_reply.started":"2024-09-17T14:35:20.081702Z","shell.execute_reply":"2024-09-17T14:35:30.77404Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f4d89ddd38b4e8199723ad9047b9469"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e10b702a6b8a4abbb661e69651b4d501"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48e6d1a7f7884d1e8abf3b76f5efb605"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64f10eaac7e443e9842a7780d994580f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a8672861c594523be0aaa597710da65"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2879ffa3bad945799e483a6474398cef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7a82cf93ac041699ba81e9c0b94087f"}},"metadata":{}}]},{"cell_type":"code","source":"import pandas as pd\n\n# Load dataset\ndataset = pd.read_json('/kaggle/input/tax-reduction-pkp/dataset.json')","metadata":{"execution":{"iopub.status.busy":"2024-09-17T14:35:37.438619Z","iopub.execute_input":"2024-09-17T14:35:37.43942Z","iopub.status.idle":"2024-09-17T14:35:37.898265Z","shell.execute_reply.started":"2024-09-17T14:35:37.439379Z","shell.execute_reply":"2024-09-17T14:35:37.897149Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import json\n\n# Load the JSON dataset\nwith open('/kaggle/input/tax-reduction-pkp/dataset.json', 'r') as f:\n    data = json.load(f)\n\n# Write to a plain text file\nwith open('dataset.txt', 'w') as f:\n    for entry in data:\n        f.write(entry['prompt'] + '\\n')\n        f.write(entry['completion'] + '\\n\\n')","metadata":{"execution":{"iopub.status.busy":"2024-09-17T14:35:41.606664Z","iopub.execute_input":"2024-09-17T14:35:41.607534Z","iopub.status.idle":"2024-09-17T14:35:41.614398Z","shell.execute_reply.started":"2024-09-17T14:35:41.607481Z","shell.execute_reply":"2024-09-17T14:35:41.613513Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"from transformers import TextDataset, DataCollatorForLanguageModeling\n\ndef load_dataset(file_path, tokenizer, block_size=128):\n    return TextDataset(\n        tokenizer=tokenizer,\n        file_path=file_path,\n        block_size=block_size\n    )\n\ndef create_data_collator(tokenizer):\n    return DataCollatorForLanguageModeling(\n        tokenizer=tokenizer,\n        mlm=False\n    )\n\n# Prepare dataset and data collator\ntrain_dataset = load_dataset('/kaggle/working/dataset.txt', tokenizer)\ndata_collator = create_data_collator(tokenizer)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-17T14:35:45.577288Z","iopub.execute_input":"2024-09-17T14:35:45.577714Z","iopub.status.idle":"2024-09-17T14:35:59.385385Z","shell.execute_reply.started":"2024-09-17T14:35:45.577676Z","shell.execute_reply":"2024-09-17T14:35:59.384409Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import Trainer, TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    overwrite_output_dir=True,\n    num_train_epochs=30,\n    per_device_train_batch_size=4,\n    save_steps=10_000,\n    save_total_limit=2,\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-17T14:36:04.471206Z","iopub.execute_input":"2024-09-17T14:36:04.47253Z","iopub.status.idle":"2024-09-17T14:36:06.373177Z","shell.execute_reply.started":"2024-09-17T14:36:04.472484Z","shell.execute_reply":"2024-09-17T14:36:06.372365Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=train_dataset,\n)\n\ntrainer.train()\n","metadata":{"execution":{"iopub.status.busy":"2024-09-17T14:36:09.754583Z","iopub.execute_input":"2024-09-17T14:36:09.755644Z","iopub.status.idle":"2024-09-17T14:39:02.248413Z","shell.execute_reply.started":"2024-09-17T14:36:09.75559Z","shell.execute_reply":"2024-09-17T14:39:02.247562Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.18.1 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.7"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240917_143644-ixs7ydde</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ppranavkrishna1503-bangalore-institute-of-technology/huggingface/runs/ixs7ydde' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/ppranavkrishna1503-bangalore-institute-of-technology/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ppranavkrishna1503-bangalore-institute-of-technology/huggingface' target=\"_blank\">https://wandb.ai/ppranavkrishna1503-bangalore-institute-of-technology/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ppranavkrishna1503-bangalore-institute-of-technology/huggingface/runs/ixs7ydde' target=\"_blank\">https://wandb.ai/ppranavkrishna1503-bangalore-institute-of-technology/huggingface/runs/ixs7ydde</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='450' max='450' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [450/450 01:59, Epoch 30/30]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=450, training_loss=0.6597812228732639, metrics={'train_runtime': 169.8002, 'train_samples_per_second': 20.318, 'train_steps_per_second': 2.65, 'total_flos': 225364377600000.0, 'train_loss': 0.6597812228732639, 'epoch': 30.0})"},"metadata":{}}]},{"cell_type":"code","source":"model.save_pretrained('/kaggle/working/trained_gpt2')\ntokenizer.save_pretrained('/kaggle/working/trained_gpt2')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import GPT2Tokenizer, GPT2LMHeadModel\n\n# Load the fine-tuned model and tokenizer\nmodel_path = '/kaggle/working/trained_gpt2'  # Path to your fine-tuned model\ntokenizer = GPT2Tokenizer.from_pretrained(model_path)\nmodel = GPT2LMHeadModel.from_pretrained(model_path)\n\n# Define a function to generate a response from the model\ndef generate_response(question, max_length=100, num_return_sequences=1):\n    inputs = tokenizer.encode(question, return_tensors='pt')\n    outputs = model.generate(\n        inputs,\n        max_length=max_length,\n        num_return_sequences=num_return_sequences,\n        no_repeat_ngram_size=2,\n        early_stopping=True\n    )\n    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return response\n\n# Example question to test the model\nquestion = \"What are some tax-saving investment options under Section 80C?\"\nresponse = generate_response(question)\n\nprint(\"Question:\", question)\nprint(\"Response:\", response)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import GPT2Tokenizer, GPT2LMHeadModel\n\n# Load the fine-tuned model and tokenizer\nmodel_path = '/kaggle/working/trained_gpt2'  # Path to your fine-tuned model\ntokenizer = GPT2Tokenizer.from_pretrained(model_path)\nmodel = GPT2LMHeadModel.from_pretrained(model_path)\n\n# Define a function to generate a response from the model\ndef generate_response(question, max_length=100, num_return_sequences=1):\n    inputs = tokenizer.encode(question, return_tensors='pt')\n    outputs = model.generate(\n        inputs,\n        max_length=max_length,\n        num_return_sequences=num_return_sequences,\n        no_repeat_ngram_size=2,\n        early_stopping=True,\n        temperature=0.2,  # Adjust temperature for more controlled outputs\n        top_k=50,         # Adjust top_k for diversity in responses\n        top_p=0.5         # Adjust top_p for diversity in responses\n    )\n    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return response\n\n# Example question to test the model\nquestion = \"What is Section 80TTB and its benefits for senior citizens?\"\nresponse = generate_response(question)\n\nprint(\"Question:\", question)\nprint(\"Response:\", response)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shutil\n\nshutil.make_archive('/kaggle/working/trained_gpt2', 'zip', '/kaggle/working/trained_gpt2')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}