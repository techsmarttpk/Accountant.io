{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9410484,"sourceType":"datasetVersion","datasetId":5714317}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/pranavkrishnaprathap/gpt2training?scriptVersionId=197029735\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"!pip install transformers","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-17T14:34:55.04732Z","iopub.execute_input":"2024-09-17T14:34:55.047666Z","iopub.status.idle":"2024-09-17T14:35:09.92581Z","shell.execute_reply.started":"2024-09-17T14:34:55.047628Z","shell.execute_reply":"2024-09-17T14:35:09.924825Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.44.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.24.6)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.4)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.7.4)\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import GPT2Tokenizer, GPT2LMHeadModel\n\n# Load pre-trained model and tokenizer\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\nmodel = GPT2LMHeadModel.from_pretrained('gpt2')\n","metadata":{"execution":{"iopub.status.busy":"2024-09-17T14:35:20.08105Z","iopub.execute_input":"2024-09-17T14:35:20.081746Z","iopub.status.idle":"2024-09-17T14:35:30.774994Z","shell.execute_reply.started":"2024-09-17T14:35:20.081702Z","shell.execute_reply":"2024-09-17T14:35:30.77404Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f4d89ddd38b4e8199723ad9047b9469"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e10b702a6b8a4abbb661e69651b4d501"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48e6d1a7f7884d1e8abf3b76f5efb605"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64f10eaac7e443e9842a7780d994580f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a8672861c594523be0aaa597710da65"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2879ffa3bad945799e483a6474398cef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7a82cf93ac041699ba81e9c0b94087f"}},"metadata":{}}]},{"cell_type":"code","source":"import pandas as pd\n\n# Load dataset\ndataset = pd.read_json('/kaggle/input/tax-reduction-pkp/dataset.json')","metadata":{"execution":{"iopub.status.busy":"2024-09-17T14:35:37.438619Z","iopub.execute_input":"2024-09-17T14:35:37.43942Z","iopub.status.idle":"2024-09-17T14:35:37.898265Z","shell.execute_reply.started":"2024-09-17T14:35:37.439379Z","shell.execute_reply":"2024-09-17T14:35:37.897149Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import json\n\n# Load the JSON dataset\nwith open('/kaggle/input/tax-reduction-pkp/dataset.json', 'r') as f:\n    data = json.load(f)\n\n# Write to a plain text file\nwith open('dataset.txt', 'w') as f:\n    for entry in data:\n        f.write(entry['prompt'] + '\\n')\n        f.write(entry['completion'] + '\\n\\n')","metadata":{"execution":{"iopub.status.busy":"2024-09-17T14:35:41.606664Z","iopub.execute_input":"2024-09-17T14:35:41.607534Z","iopub.status.idle":"2024-09-17T14:35:41.614398Z","shell.execute_reply.started":"2024-09-17T14:35:41.607481Z","shell.execute_reply":"2024-09-17T14:35:41.613513Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"from transformers import TextDataset, DataCollatorForLanguageModeling\n\ndef load_dataset(file_path, tokenizer, block_size=128):\n    return TextDataset(\n        tokenizer=tokenizer,\n        file_path=file_path,\n        block_size=block_size\n    )\n\ndef create_data_collator(tokenizer):\n    return DataCollatorForLanguageModeling(\n        tokenizer=tokenizer,\n        mlm=False\n    )\n\n# Prepare dataset and data collator\ntrain_dataset = load_dataset('/kaggle/working/dataset.txt', tokenizer)\ndata_collator = create_data_collator(tokenizer)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-17T14:35:45.577288Z","iopub.execute_input":"2024-09-17T14:35:45.577714Z","iopub.status.idle":"2024-09-17T14:35:59.385385Z","shell.execute_reply.started":"2024-09-17T14:35:45.577676Z","shell.execute_reply":"2024-09-17T14:35:59.384409Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the  Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import Trainer, TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    overwrite_output_dir=True,\n    num_train_epochs=30,\n    per_device_train_batch_size=4,\n    save_steps=10_000,\n    save_total_limit=2,\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-17T14:36:04.471206Z","iopub.execute_input":"2024-09-17T14:36:04.47253Z","iopub.status.idle":"2024-09-17T14:36:06.373177Z","shell.execute_reply.started":"2024-09-17T14:36:04.472484Z","shell.execute_reply":"2024-09-17T14:36:06.372365Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=train_dataset,\n)\n\ntrainer.train()\n","metadata":{"execution":{"iopub.status.busy":"2024-09-17T14:36:09.754583Z","iopub.execute_input":"2024-09-17T14:36:09.755644Z","iopub.status.idle":"2024-09-17T14:39:02.248413Z","shell.execute_reply.started":"2024-09-17T14:36:09.75559Z","shell.execute_reply":"2024-09-17T14:39:02.247562Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.18.1 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.7"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240917_143644-ixs7ydde</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ppranavkrishna1503-bangalore-institute-of-technology/huggingface/runs/ixs7ydde' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/ppranavkrishna1503-bangalore-institute-of-technology/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ppranavkrishna1503-bangalore-institute-of-technology/huggingface' target=\"_blank\">https://wandb.ai/ppranavkrishna1503-bangalore-institute-of-technology/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ppranavkrishna1503-bangalore-institute-of-technology/huggingface/runs/ixs7ydde' target=\"_blank\">https://wandb.ai/ppranavkrishna1503-bangalore-institute-of-technology/huggingface/runs/ixs7ydde</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='450' max='450' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [450/450 01:59, Epoch 30/30]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=450, training_loss=0.6597812228732639, metrics={'train_runtime': 169.8002, 'train_samples_per_second': 20.318, 'train_steps_per_second': 2.65, 'total_flos': 225364377600000.0, 'train_loss': 0.6597812228732639, 'epoch': 30.0})"},"metadata":{}}]},{"cell_type":"code","source":"model.save_pretrained('/kaggle/working/trained_gpt2')\ntokenizer.save_pretrained('/kaggle/working/trained_gpt2')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import GPT2Tokenizer, GPT2LMHeadModel\n\n# Load the fine-tuned model and tokenizer\nmodel_path = '/kaggle/working/trained_gpt2'  # Path to your fine-tuned model\ntokenizer = GPT2Tokenizer.from_pretrained(model_path)\nmodel = GPT2LMHeadModel.from_pretrained(model_path)\n\n# Define a function to generate a response from the model\ndef generate_response(question, max_length=100, num_return_sequences=1):\n    inputs = tokenizer.encode(question, return_tensors='pt')\n    outputs = model.generate(\n        inputs,\n        max_length=max_length,\n        num_return_sequences=num_return_sequences,\n        no_repeat_ngram_size=2,\n        early_stopping=True\n    )\n    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return response\n\n# Example question to test the model\nquestion = \"What are some tax-saving investment options under Section 80C?\"\nresponse = generate_response(question)\n\nprint(\"Question:\", question)\nprint(\"Response:\", response)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import GPT2Tokenizer, GPT2LMHeadModel\n\n# Load the fine-tuned model and tokenizer\nmodel_path = '/kaggle/working/trained_gpt2'  # Path to your fine-tuned model\ntokenizer = GPT2Tokenizer.from_pretrained(model_path)\nmodel = GPT2LMHeadModel.from_pretrained(model_path)\n\n# Define a function to generate a response from the model\ndef generate_response(question, max_length=100, num_return_sequences=1):\n    inputs = tokenizer.encode(question, return_tensors='pt')\n    outputs = model.generate(\n        inputs,\n        max_length=max_length,\n        num_return_sequences=num_return_sequences,\n        no_repeat_ngram_size=2,\n        early_stopping=True,\n        temperature=0.2,  # Adjust temperature for more controlled outputs\n        top_k=50,         # Adjust top_k for diversity in responses\n        top_p=0.5         # Adjust top_p for diversity in responses\n    )\n    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return response\n\n# Example question to test the model\nquestion = \"What is Section 80TTB and its benefits for senior citizens?\"\nresponse = generate_response(question)\n\nprint(\"Question:\", question)\nprint(\"Response:\", response)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shutil\n\nshutil.make_archive('/kaggle/working/trained_gpt2', 'zip', '/kaggle/working/trained_gpt2')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}